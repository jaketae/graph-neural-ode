{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from torchgde import GCN, GCNLayer, GDEFunc, ODEBlock, PerformanceContainer, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.data\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised node classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces `GDEs` as a general high-performance model for graph structured data. Notebook `01_node_classification_gde` is designed from the ground up as an introduction to GDEs and therefore contains ample comments to provide insights on some of our design choices. To be accessible to practicioners/researchers without prior experience on GNNs, we discuss some features of `dgl` as well, one of the PyTorch ecosystems for geometric deep learning.\n",
    "\n",
    "No prior experience with `Neural ODEs` is required, though we refer to the original repo [torchdiffeq](https://github.com/rtqichen/torchdiffeq) as a complete reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "# seed for repeatability\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dgl offers convenient access to GNN benchmark datasets via `dgl.data`...\n",
    "# other standard datasets (e.g. Citeseer / Pubmed) are also accessible via the dgl.data\n",
    "# API. The rest of the notebook is compatible with Cora / Citeseer / Pubmed with minimal\n",
    "# modification required.\n",
    "data = dgl.data.CoraGraphDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cora is a node-classification datasets with 2708 nodes\n",
    "X = torch.FloatTensor(data[0].ndata[\"feat\"]).to(device)\n",
    "Y = torch.LongTensor(data[0].ndata[\"label\"]).to(device)\n",
    "\n",
    "# In transductive semi-supervised node classification tasks on graphs, the model has access to all\n",
    "# node features but only a masked subset of the labels\n",
    "train_mask = torch.BoolTensor(data[0].ndata[\"train_mask\"])\n",
    "val_mask = torch.BoolTensor(data[0].ndata[\"val_mask\"])\n",
    "test_mask = torch.BoolTensor(data[0].ndata[\"test_mask\"])\n",
    "\n",
    "num_feats = X.shape[1]\n",
    "n_classes = data.num_classes\n",
    "\n",
    "# 140 training samples, 300 validation, 1000 test\n",
    "n_classes, train_mask.sum().item(), val_mask.sum().item(), test_mask.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add self-edge for each node\n",
    "# g = data[0].graph\n",
    "# g.remove_edges_from(nx.selfloop_edges(g))\n",
    "# g.add_edges_from(zip(g.nodes(), g.nodes()))\n",
    "# g = dgl.DGLGraph(g)\n",
    "# edges = g.edges()\n",
    "# n_edges = g.number_of_edges()\n",
    "\n",
    "# n_edges\n",
    "g = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute diagonal of normalization matrix D according to standard formula\n",
    "degs = g.in_degrees().float()\n",
    "norm = torch.pow(degs, -0.5)\n",
    "norm[torch.isinf(norm)] = 0\n",
    "# add to dgl.Graph in order for the norm to be accessible at training time\n",
    "g.ndata[\"norm\"] = norm.unsqueeze(1)  # .to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = g.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Differential Equations (GDEs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Neural ODEs, GDEs require specification of an ODE function (`ODEFunc`), representing the set of layers that will be called repeatedly by the ODE solver, as well as an ODE block (`ODEBlock`), tasked with calling the ODE solver on the ODE function. The ODEFunc is passed to the ODEBlock at initialization.\n",
    "\n",
    "We introduce the convolutional variant of GDEs, `GCDEs`. The only difference resides in the type of GNN layer utilized in the ODEFunc.\n",
    "\n",
    "For adaptive step GDEs (dopri5) we increase the hidden dimension to 64 to reduce the stiffness of the ODE and therefore the number of ODEFunc evaluations (`NFE`: Number Function Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GCDEs, the ODEFunc is specified by two GCN layers. Softplus is used as activation. Smoother activations\n",
    "# have been observed to help avoid numerical instability and reduce stiffness of the ODE described\n",
    "# by a repeated call to the ODEFunc. High dropout improves performance on transductive node classification\n",
    "# tasks due to their small training sets. GDEs can take advantage of this property due to their 'deeper'\n",
    "# computational graph. NOTE: too much dropout increases stiffness and therefore NFEs\n",
    "gnn = nn.Sequential(\n",
    "    GCNLayer(g=g, in_feats=64, out_feats=64, activation=nn.Softplus(), dropout=0.9),\n",
    "    GCNLayer(g=g, in_feats=64, out_feats=64, activation=None, dropout=0.9),\n",
    ").to(device)\n",
    "\n",
    "gdefunc = GDEFunc(gnn)\n",
    "\n",
    "# dopri5 is an adaptive step solver and will call `gdefunc` several times to ensure correctness up to pre-specified\n",
    "# tolerance levels. rk4 will call the func 4 times. As suggested in the original Neural ODE paper and as observed during internal tests, lower tolerances\n",
    "# are sufficient for classification tasks.\n",
    "gde = ODEBlock(odefunc=gdefunc, method=\"rk4\", atol=1e-3, rtol=1e-4, adjoint=False).to(device)\n",
    "\n",
    "\n",
    "m = nn.Sequential(\n",
    "    GCNLayer(g=g, in_feats=num_feats, out_feats=64, activation=F.relu, dropout=0.4),\n",
    "    gde,\n",
    "    GCNLayer(g=g, in_feats=64, out_feats=n_classes, activation=None, dropout=0.0),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdefunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in m:\n",
    "    print(count_parameters(layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "We use standard hyperparameters for GCNs, namely `1e-2` learning rate and `5e-4` weight decay. For a fair comparison, GDE-dpr5 should be evaluated against deeper GCN models (GCNs with 4+ layers). This is because datasets such as Cora penalize deeper models due to small training sets and thus need for very strong regularizers. GDE-rk4, whose ODEFunc is evaluated only 4 times, should be compared with shallower GCN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(m.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "logger = PerformanceContainer(\n",
    "    data={\n",
    "        \"train_loss\": [],\n",
    "        \"train_accuracy\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_accuracy\": [],\n",
    "        \"forward_time\": [],\n",
    "        \"backward_time\": [],\n",
    "        \"nfe\": [],\n",
    "    }\n",
    ")\n",
    "steps = 3000\n",
    "verbose_step = 1\n",
    "num_grad_steps = 0\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in range(steps):  # looping over epochs\n",
    "    m.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    outputs = m(X)\n",
    "    f_time = time.time() - start_time\n",
    "\n",
    "    nfe = m._modules[\"1\"].odefunc.nfe\n",
    "\n",
    "    y_pred = outputs\n",
    "\n",
    "    loss = criterion(y_pred[train_mask], Y[train_mask])\n",
    "    opt.zero_grad()\n",
    "\n",
    "    start_time = time.time()\n",
    "    loss.backward()\n",
    "    b_time = time.time() - start_time\n",
    "\n",
    "    opt.step()\n",
    "    num_grad_steps += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        m.eval()\n",
    "\n",
    "        # calculating outputs again with zeroed dropout\n",
    "        y_pred = m(X)\n",
    "        m._modules[\"1\"].odefunc.nfe = 0\n",
    "\n",
    "        train_loss = loss.item()\n",
    "        train_acc = accuracy(y_pred[train_mask], Y[train_mask]).item()\n",
    "        test_acc = accuracy(y_pred[test_mask], Y[test_mask]).item()\n",
    "        test_loss = criterion(y_pred[test_mask], Y[test_mask]).item()\n",
    "        logger.deep_update(\n",
    "            logger.data,\n",
    "            dict(\n",
    "                train_loss=[train_loss],\n",
    "                train_accuracy=[train_acc],\n",
    "                test_loss=[test_loss],\n",
    "                test_accuracy=[test_acc],\n",
    "                nfe=[nfe],\n",
    "                forward_time=[f_time],\n",
    "                backward_time=[b_time],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    if num_grad_steps % verbose_step == 0:\n",
    "        print(\n",
    "            \"[{}], Loss: {:3.3f}, Train Accuracy: {:3.3f}, Test Accuracy: {:3.3f}, NFE: {}\".format(\n",
    "                num_grad_steps, train_loss, train_acc, test_acc, nfe\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(logger.data[\"train_loss\"])\n",
    "plt.plot(logger.data[\"test_loss\"])\n",
    "plt.legend([\"Train loss\", \"Test loss\"])\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(logger.data[\"train_accuracy\"])\n",
    "plt.plot(logger.data[\"test_accuracy\"])\n",
    "plt.legend([\"Train accuracy\", \"Test accuracy\"])\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(logger.data[\"forward_time\"])\n",
    "plt.plot(logger.data[\"backward_time\"])\n",
    "plt.legend([\"Forward time\", \"Backward time\"])\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(logger.data[\"nfe\"], marker=\"o\", linewidth=0.1, markersize=1)\n",
    "plt.legend([\"NFE\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in observing the training behavior of regular GCNs of different layer depths. Below is a training loop\n",
    "that will collect metrics for GCNs of layers `1, 3, 5, 7`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loggers = []\n",
    "\n",
    "for n_layers in range(2, 9, 2):\n",
    "    gcn = GCN(\n",
    "        num_layers=n_layers,\n",
    "        g=g,\n",
    "        in_feats=1433,\n",
    "        hidden_feats=64,\n",
    "        out_feats=7,\n",
    "        activation=F.relu,\n",
    "        dropout=0.9,\n",
    "    ).to(device)\n",
    "    opt = torch.optim.Adam(gcn.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    num_grad_steps = 3000\n",
    "\n",
    "    gcn_logger = PerformanceContainer(\n",
    "        data={\n",
    "            \"train_loss\": [],\n",
    "            \"train_accuracy\": [],\n",
    "            \"test_loss\": [],\n",
    "            \"test_accuracy\": [],\n",
    "            \"forward_time\": [],\n",
    "            \"backward_time\": [],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for i in range(num_grad_steps):  # looping over epochs\n",
    "        start_time = time.time()\n",
    "        outputs = gcn(X)\n",
    "        f_time = time.time() - start_time\n",
    "\n",
    "        y_pred = outputs\n",
    "\n",
    "        loss = criterion(y_pred[train_mask], Y[train_mask])\n",
    "        opt.zero_grad()\n",
    "\n",
    "        start_time = time.time()\n",
    "        loss.backward()\n",
    "        b_time = time.time() - start_time\n",
    "\n",
    "        opt.step()\n",
    "        num_grad_steps += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gcn.eval()\n",
    "            outputs = gcn(X)\n",
    "            y_pred = outputs\n",
    "\n",
    "            train_loss = loss.item()\n",
    "            train_acc = accuracy(y_pred[train_mask], Y[train_mask]).item()\n",
    "            test_acc = accuracy(y_pred[test_mask], Y[test_mask]).item()\n",
    "            test_loss = criterion(y_pred[test_mask], Y[test_mask]).item()\n",
    "\n",
    "            gcn_logger.deep_update(\n",
    "                gcn_logger.data,\n",
    "                dict(\n",
    "                    train_loss=[train_loss],\n",
    "                    train_accuracy=[train_acc],\n",
    "                    test_loss=[test_loss],\n",
    "                    test_accuracy=[test_acc],\n",
    "                    forward_time=[f_time],\n",
    "                    backward_time=[b_time],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    loggers.append((gcn_logger, n_layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCDEs are shown to be as performing as GDEs, while being deeper and not requiring selection of number of layers. They are observed to converge faster; the high degree of noise present is due to the aggressive dropout setting, used to exploit the increased depth of GCDEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_nfe = int(np.array(logger.data[\"nfe\"]).mean())\n",
    "layer_range = range(2, 9, 2)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(logger.data[\"train_loss\"])\n",
    "for logger_gcn, l in loggers:\n",
    "    plt.plot(logger_gcn.data[\"train_loss\"])\n",
    "plt.title(\"Training loss\")\n",
    "plt.legend([f\"GCDE{mean_nfe}\"] + [f\"GCN{i}\" for i in layer_range])\n",
    "plt.ylim(0, 2)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(logger.data[\"test_loss\"])\n",
    "for logger_gcn, l in loggers:\n",
    "    plt.plot(logger_gcn.data[\"test_loss\"])\n",
    "plt.title(\"Test loss\")\n",
    "plt.legend([f\"GCDE{mean_nfe}\"] + [f\"GCN{i}\" for i in layer_range])\n",
    "plt.ylim(0, 3)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(logger.data[\"train_accuracy\"])\n",
    "for logger_gcn, l in loggers:\n",
    "    plt.plot(logger_gcn.data[\"train_accuracy\"])\n",
    "plt.title(\"Train accuracy\")\n",
    "plt.legend([f\"GCDE{mean_nfe}\"] + [f\"GCN{i}\" for i in layer_range])\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(logger.data[\"test_accuracy\"])\n",
    "for logger_gcn, l in loggers:\n",
    "    plt.plot(logger_gcn.data[\"test_accuracy\"])\n",
    "plt.title(\"Test accuracy\")\n",
    "plt.legend([f\"GCDE{mean_nfe}\"] + [f\"GCN{i}\" for i in layer_range]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots below show how forward time for GCDEs is roughly 6 times that of GCN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftime = logger.data[\"forward_time\"]\n",
    "btime = logger.data[\"backward_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(ftime)\n",
    "for logger_gcn, l in loggers:\n",
    "    plt.plot(logger_gcn.data[\"forward_time\"])\n",
    "plt.title(\"Forward time (s)\")\n",
    "plt.legend([\"GCDE\"] + [f\"GCN{i}\" for i in layer_range])\n",
    "plt.ylim(0, 0.006)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(btime)\n",
    "for logger_gcn, l in loggers:\n",
    "    plt.plot(logger_gcn.data[\"backward_time\"])\n",
    "plt.title(\"Backward time (s)\")\n",
    "plt.legend([\"GCDE\"] + [f\"GCN{i}\" for i in layer_range]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
